# Go LLM Proxy

A maintainable Go-based proxy for LLMs that allows JetBrains IDEs to communicate with various AI providers using the Ollama API format.

## ğŸ—ï¸ Project Structure

```
go-llm-proxy/
â”œâ”€â”€ bin/                         # Compiled binaries
â”‚   â”œâ”€â”€ llm-proxy
â”‚   â”œâ”€â”€ llm-proxy-linux
â”‚   â”œâ”€â”€ llm-proxy-macos
â”‚   â””â”€â”€ llm-proxy-windows.exe
â”œâ”€â”€ cmd/llm-proxy/               # Main application entry point
â”‚   â””â”€â”€ main.go
â”œâ”€â”€ internal/                    # Internal packages
â”‚   â”œâ”€â”€ config/                  # Configuration management
â”‚   â”‚   â””â”€â”€ config.go
â”‚   â”œâ”€â”€ models/                  # Model registry and management
â”‚   â”‚   â””â”€â”€ models.go
â”‚   â”œâ”€â”€ backend/                 # Backend abstraction layer
â”‚   â”‚   â””â”€â”€ backend.go
â”‚   â”œâ”€â”€ proxy/                   # Main proxy server logic
â”‚   â”‚   â””â”€â”€ proxy.go
â”‚   â”œâ”€â”€ streaming/               # Streaming response handling
â”‚   â”‚   â””â”€â”€ streaming.go
â”‚   â””â”€â”€ types/                   # Shared type definitions
â”‚       â””â”€â”€ types.go
â”œâ”€â”€ pkg/                         # Public packages
â”‚   â”œâ”€â”€ anthropic/               # Anthropic Claude integration
â”‚   â”‚   â””â”€â”€ anthropic_backend.go
â”‚   â””â”€â”€ openai/                  # OpenAI GPT integration
â”‚       â””â”€â”€ openai_backend.go
â”œâ”€â”€ test/                        # Test files
â”‚   â”œâ”€â”€ unit/                    # Unit tests
â”‚   â”‚   â”œâ”€â”€ config_test.go
â”‚   â”‚   â”œâ”€â”€ model_management_test.go
â”‚   â”‚   â”œâ”€â”€ ollama_api_test.go
â”‚   â”‚   â””â”€â”€ proxy_test.go
â”‚   â””â”€â”€ integration/             # Integration tests
â”‚       â””â”€â”€ integration_test.go
â”œâ”€â”€ scripts/                     # Build and utility scripts
â”‚   â”œâ”€â”€ setup.sh
â”‚   â””â”€â”€ test_proxy.sh
â”œâ”€â”€ examples/                    # Usage examples
â”‚   â””â”€â”€ main.go
â”œâ”€â”€ docs/                        # Documentation
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ TEST_SUMMARY.md
â”œâ”€â”€ .env.example                 # Environment variables template
â”œâ”€â”€ go.mod                       # Go module definition
â”œâ”€â”€ go.sum                       # Go module checksums
â””â”€â”€ Makefile                     # Build automation
```

## ğŸš€ Quick Start

### Prerequisites
- Go 1.21 or later
- API keys for Anthropic and/or OpenAI

### Installation

1. **Clone the repository:**
   ```bash
   git clone <repository-url>
   cd go-llm-proxy
   ```

2. **Install dependencies:**
   ```bash
   make deps
   ```

3. **Set up environment variables:**
   ```bash
   cp .env.example .env
   # Edit .env with your API keys
   ```

4. **Build the proxy:**
   ```bash
   make build
   ```

5. **Run the proxy:**
   ```bash
   ./bin/llm-proxy
   ```

## ğŸ”§ Usage

### Building the Proxy

```bash
# Build for current platform
make build

# Build for all platforms
make build-all

# Clean build artifacts
make clean
```

### Running the Proxy

```bash
# Run the built binary
./bin/llm-proxy

# Or run directly without building
make run
```

### Testing

```bash
# Run all tests
make test

# Run with verbose output
make test-verbose

# Run with coverage
make test-coverage

# Run specific test suites
make test-api          # API compatibility tests
make test-integration  # Integration tests
make test-models       # Model management tests
make test-config       # Configuration tests
```

## ğŸ“ Source Files

The main source files are organized as follows:

### Core Application
- **`cmd/llm-proxy/main.go`** - Main application entry point

### Internal Packages
- **`internal/types/types.go`** - Shared type definitions and interfaces
- **`internal/config/config.go`** - Configuration management
- **`internal/models/models.go`** - Model registry and management
- **`internal/backend/backend.go`** - Backend abstraction layer
- **`internal/proxy/proxy.go`** - Main proxy server logic
- **`internal/streaming/streaming.go`** - Streaming response handling

### Public Packages
- **`pkg/anthropic/anthropic_backend.go`** - Anthropic Claude integration
- **`pkg/openai/openai_backend.go`** - OpenAI GPT integration

### Test Files

All test files are organized by type:

#### Unit Tests (`test/unit/`)
- **`config_test.go`** - Configuration tests
- **`model_management_test.go`** - Model management tests
- **`ollama_api_test.go`** - Ollama API compatibility tests
- **`proxy_test.go`** - Basic proxy tests

#### Integration Tests (`test/integration/`)
- **`integration_test.go`** - End-to-end integration tests

## ğŸ§ª Testing

The project includes comprehensive tests:

- **Unit Tests** - Individual component testing
- **Integration Tests** - End-to-end workflow testing
- **API Compatibility Tests** - Ollama API compliance verification
- **Model Management Tests** - Model registry functionality
- **Configuration Tests** - Environment variable handling

See `TEST_SUMMARY.md` for detailed test documentation.

## ğŸ”§ Configuration

The proxy can be configured via environment variables:

```bash
# Server configuration
PORT=11434
GIN_MODE=release

# API Keys
ANTHROPIC_API_KEY=your_anthropic_key_here
OPENAI_API_KEY=your_openai_key_here

# Model configuration
DEFAULT_MAX_TOKENS=4096
STREAMING_CHUNK_SIZE=3
STREAMING_DELAY_MS=50
```

## ğŸ¯ Features

- **Ollama API Compatibility** - Full compatibility with Ollama API format
- **JetBrains IDE Support** - Works seamlessly with GoLand AI Assistant
- **Multi-Backend Support** - Anthropic and OpenAI backends
- **Streaming Support** - Both streaming and non-streaming responses
- **Model Management** - Dynamic model addition/removal
- **CORS Support** - Cross-origin request handling
- **Comprehensive Testing** - Full test coverage

## ğŸ“š Documentation

- **`README.md`** - This file (main documentation)
- **`TEST_SUMMARY.md`** - Comprehensive test documentation
- **`examples/main.go`** - Usage examples
- **`scripts/`** - Build and utility scripts

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Run the test suite: `make test`
6. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ†˜ Support

For issues and questions:
1. Check the documentation
2. Run the test suite to verify setup
3. Check the logs for error messages
4. Open an issue on GitHub